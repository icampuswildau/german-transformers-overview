# Pre-Trained Transformer language models

The given tables show Transformer language models that have been trained specifically for the German language.
This kind of models can be further trained in general (continue generic `pre-training` or shifting to terms of a domain), 
but can also be optimized for specific tasks (`fine-tuning`).

The columns `Type, Raw data, Parameters, Vocab & Tokenizer` and `Time` correspond to the most important 
elements that determine successful model training.

|                                                                    Name                                                                   |    Type    |                                                                                                                                                                                          Raw data | Parameters                                                                                                                                                                                                                                             | Vocab & Tokenizer                                                                                                                                                                                                                                                                                                                                                                           |               Time              |   License  |
|:-----------------------------------------------------------------------------------------------------------------------------------------:|:----------:|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------:|:----------:|
| [benjamin/gerpt2](https://huggingface.co/benjamin/gerpt2)                                                                                 |    GPT2    |                                                                                                                                                        - CommonCrawl = 67 GB <br>  Σ = 67 GB <br> | - 256 batch size <br> - 0.005 learning rate ("OneCycle") <br> - 1024 max sequence length <br> - 7 epochs <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers <br> - special: weights initialized from english GPT2 model             | - Byte Level Byte Pair Encoding <br> - based on 5% random raw data <br> - 50.257 vocab size <br> - contains "Umlaute" <br> - cased <br> - Preprocessing: ?                                                                                                                                                                                                                                  |        6 days on TPUv3-8        |     MIT    |
| [benjamin/gerpt2-large](https://huggingface.co/benjamin/gerpt2-large)                                                                     |    GPT2    |                                                                                                                                                                                   - see row above | - 256 batch size <br> - 0.005 learning rate ("OneCycle") <br> - 1024 max sequence length <br> - 2 epochs <br> - 1280 embeddings dimensionality <br> - 20 heads <br> - 36 layers <br> - special: weights initialized from english GPT2 model            | - see row above                                                                                                                                                                                                                                                                                                                                                                             |        12 days on TPUv3-8       |     MIT    |
| [bert-base-german-cased](https://huggingface.co/bert-base-german-cased)                                                                   |    BERT    |                                                                                             - Wikipedia = 6 GB <br>  - OpenLegalData = 2.4 GB <br>  - News articles = 3.6 GB <br>  Σ = 12 GB <br> | - 1024 batch size <br> - 0.0001 learning rate <br>  - 810.000 steps with 128 max sequence length <br>  - 30.0000 steps with 512 max sequence length <br> - 10.0000 warmup steps <br> - 768 embeddings dimensionality <br>  - 12 heads <br> - 12 layers | - WordPiece based on SentencePiece <br> - 30.000 vocab size <br> - contains "Umlaute" <br> - cased <br> - Sentence Segmentation via spacy <br>  - Preprocessing by "tailored scripts" (so further information given) <br>                                                                                                                                                                   |        9 days on TPUv2-8        |     MIT    |
| [dbmdz/bert-base-german-cased](https://huggingface.co/dbmdz/bert-base-german-cased)                                                       |    BERT    | - Wikipedia = ? GB <br>  - EU Bookshop = ? GB <br>  - OpenSubtitles = ? GB <br>  - CommonCrawl = ? GB <br>  - ParaCrawl = ? GB <br>  - News articles (WMT Crawl 2018) = ? GB <br>  Σ = 16 GB <br> | - ? batch size <br> - ? learning rate <br> - 512 max sequence length <br> - 1.500.000 steps <br> - 100 warmup setps <br> - 768 embeddings dimensionality <br>  - 12 heads <br> - 12 layers                                                             | - WordPiece based on SentencePiece <br> - based on 100.000 random sentences from raw data <br>  - 31.102 vocab size <br> - cased <br> - contains "Umlaute" <br>  - Sentence Segmentation via spacy <br>  - Preprocessing:](https://github.com/allenai/scibert /blob/master/scripts/build_pretrain_corpus.py (author says it is based on SciBERT)                                             |                ?                |     MIT    |
| [dbmdz/bert-base-german-uncased](https://huggingface.co/dbmdz/bert-base-german-uncased)                                                   |    BERT    |                                                                                                                                                                                   - see row above | - see row above                                                                                                                                                                                                                                        | - see row above <br> - uncased instead of cased <br> - Preprocessing: see row above + lowercasing of texts                                                                                                                                                                                                                                                                                  |          see row above          |     MIT    |
| [dbmdz/bert-base-german-europeana-cased](https://huggingface.co/dbmdz/bert-base-german-europeana-cased)                                   |    BERT    |                                                                                                                                               - Europeana Newspapers = 51 GB <br>  Σ = 51 GB <br> | - 128 batch size <br> - 0.0001 learning rate <br>  - 512 max sequence length <br> - 3.000.0000 steps <br> - 10.0000 warmup steps <br> - 75 max predictions per sequence <br>  - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers         | - WordPiece based on tokenizers lib <br> - 32.000 vocab size <br> - cased <br> - contains "Umlaute" <br> - Sentence Segmentation via NLTK <br> - Preprocessing: filtered Europeana corpus based on language tag                                                                                                                                                                             |        ? days on TPUv3-8        |     MIT    |
| [dbmdz/bert-base-german-europeana-uncased](https://huggingface.co/dbmdz/bert-base-german-europeana-uncased)                               |    BERT    |                                                                                                                                                                                   - see row above | - see row above                                                                                                                                                                                                                                        | - see row above <br> - uncased instead of cased                                                                                                                                                                                                                                                                                                                                             |          see row above          |     MIT    |
| [dbmdz/electra-base-german-europeana-cased-discriminator](https://huggingface.co/dbmdz/electra-base-german-europeana-cased-discriminator) |   ELECTRA  |                                                                                                                                                     - Europeana Newspapers = 51 GB <br> Σ = 51 GB | - 256 batch size <br> - 0.0003 learning rate <br> - 1.000.000 steps <br> - 10.000 warmup steps <br> - 79 max predictions per sequence <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers                                            | - WordPiece based on tokenizers lib <br>  - 32.000 vocab size <br>  - cased <br>  - contains "Umlaute" <br>  - Sentence Segmentation via NLTK <br>  - Preprocessing: filtered Europeana corpus based on language tag                                                                                                                                                                        |        8 days on TPUv3-8        |     MIT    |
| [dbmdz/german-gpt2](https://huggingface.co/dbmdz/german-gpt2)                                                                             |    GPT2    | - Wikipedia = ? GB <br>  - EU Bookshop = ? GB <br>  - OpenSubtitles = ? GB <br>  - CommonCrawl = ? GB <br>  - ParaCrawl = ? GB <br>  - News articles (WMT Crawl 2018) = ? GB <br>  Σ = 16 GB <br> | - ? batch size <br> - ? learning rate <br>  - 1024 max sequence length <br> - 3 epochs <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers                                                                                           | - Byte Level Byte Pair Encoding based on tokenizers lib <br>  - 52.000 vocab size <br> - cased <br> - contains "Umlaute" <br>  - Sentence Segmentation: ? <br>  - Preprocessing: ?                                                                                                                                                                                                          |                ?                |     MIT    |
| [deepset/gbert-base](https://huggingface.co/deepset/gbert-base)                                                                           |    BERT    |                                                                             - OSCAR = 145 GB <br>  - Wikipedia = 6 GB <br>  - OPUS = 10 GB <br>  - OpenLegalData = 2.4 GB <br>  Σ = 163.4 GB <br> | - 128 batch size <br> - 0.0001 learning rate <br> - 512 max sequence length <br> - 4.000.000 steps <br> - 10.000 warmup steps <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers                                                    | - WordPiece* based on ? <br> - 31.102 vocab size <br> - cased <br> - contains "Umlaute" <br> - Sentence Segmentation: ? <br> - Preprocessing: ? <br> - Tried to reduce biases via removal of keywords in raw data                                                                                                                                                                           |        7 days on TPUv3-8        |     MIT    |
| [deepset/gbert-large](https://huggingface.co/deepset/gbert-large)                                                                         |    BERT    |                                                                                                                                                                                   - see row above | - 2048 batch size <br> - 0.0001 learning rate <br> - 512 max sequence length <br>  - 1.000.000 steps <br> - 10.000 warmup steps <br> - 1024 embeddings dimensionality <br> - 16 heads <br> - 24 layers                                                 | - see row above                                                                                                                                                                                                                                                                                                                                                                             | 11 days on 16xTPUv3-8 (TPU pod) |     MIT    |
| [deepset/gelectra-base](https://huggingface.co/deepset/gelectra-base)                                                                     |   ELECTRA  |                                                                             - OSCAR = 145 GB <br>  - Wikipedia = 6 GB <br>  - OPUS = 10 GB <br>  - OpenLegalData = 2.4 GB <br>  Σ = 163.4 GB <br> | - 256 batch size <br> - 0.0002 learning rate <br> - 512 max sequence length <br> - 766.000 steps <br> - 10.000 warmup steps <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers <br>                                                 | - WordPiece* based on ? <br> - 31.102 vocab size <br> - cased <br> - contains "Umlaute" <br> - Sentence Segmentation: ? <br>  - Preprocessing: ? <br> - Tried to reduce biases via removal of keywords in raw data                                                                                                                                                                          |        8 days on TPUv3-8        |     MIT    |
| [deepset/gelectra-large](https://huggingface.co/deepset/gelectra-large)                                                                   |   ELECTRA  |                                                                                                                                                                                   - see row above | - 1024 batch size <br> - 0.0002 learning rate <br> - 512 max sequence length <br> - 1.000.000 steps <br> - 30.000 warmup steps <br> - 1024 embeddings dimensionality <br>  - 16 heads <br> - 24 layers                                                 | - see row above                                                                                                                                                                                                                                                                                                                                                                             |  7 days on 16xTPUv3-8 (TPU pod) |     MIT    |
| [distilbert-base-german-cased](https://huggingface.co/distilbert-base-german-cased)                                                       | DistilBERT |                                                                                                                                                                            - no description given | - ? batch size <br> - ? learning rate <br> - ? max sequence length <br> - ? steps <br> - ? warmup steps <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 6 layers                                                                           | - WordPiece based on tokenizers lib <br> - 31102 vocab size <br> - cased <br> - contains "Umlaute" <br> - Sentence Segmentation: ? <br> - Preprocessing: ?                                                                                                                                                                                                                                  |                ?                | Apache 2.0 |
| [german-nlp-group/electra-base-german-uncased](https://huggingface.co/german-nlp-group/electra-base-german-uncased)                       |   ELECTRA  |                                                                   - CCNet = 62 GB <br> - Wikipedia = 6.6 GB <br> - (Open*) Subtitles = 0.82 GB <br> - New articles = 4.1 GB <br> Σ = 86,7 GB <br> | - 256 batch size <br> - 0.0002 learning rate <br> - 512 max sequence length <br> - 766.000 steps <br> - 10.000 warmup steps <br> - 79 max predictions per sequence <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers               | - WordPiece based on tokenizers lib <br> - 32.767 vocab size <br> - uncased <br> - contains "Umlaute" <br> - Sentence Segmentation via SoMaJo <br> - Preprocessing: CCNet filtered for language score > 0.98 and only head corpus; Wikipedia not only articles but talk pages, too; 3x oversampling of wikipedia articles <br> - Tried to reduce biases via removal of keywords in raw data |        7 days on TPUv3-8        |     MIT    |
| [nikhilnagaraj/german_gpt_small](https://huggingface.co/nikhilnagaraj/german_gpt_small)                                                   |    GPT2    |                                                                                                                                                                            - no description given | - ? batch size <br> - ? learning rate <br> - 1024 max sequence length <br> - ? epochs <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers                                                                                            | - Byte Level Byte Pair Encoding* based on ? <br> - 50257 vocab size <br> - cased <br> - no "Umlaute" in tokens/subtokens (only as basic chars) <br> - Preprocessing: ?                                                                                                                                                                                                                      |                ?                |      ?     |
| [severinsimmler/bert-adapted-german-press](https://huggingface.co/severinsimmler/bert-adapted-german-press)                               |    BERT    |                                                                                                                                                                            - no description given | - ? batch size <br> - ? learning rate <br> - 512 max sequence length <br> - ? steps <br> - ? warmup steps <br>  - 768 embeddings dimensionality <br>  - 12 heads <br> - 12 layers                                                                      | - WordPiece* based on ? <br> - 119.527 vocab size <br> - cased <br> - contains "Umlaute" <br> - Preprocessing: ?                                                                                                                                                                                                                                                                            |                ?                |      ?     |
| [svalabs/ger-roberta](https://huggingface.co/svalabs/ger-roberta)                                                                         |   RoBERTa  |                                                                                                                                                                            - no description given | - ? batch size <br> - ? learning rate <br> - ? max sequence length <br> - ? steps <br> - ? warmup steps <br>  - 768 embeddings dimensionality <br>  - 12 heads <br> - 12 layers                                                                        | - Byte Level Byte Pair Encoding* based on ? <br> - 52.000 vocab size <br> - cased <br> - no "Umlaute" in tokens/subtokens (only as basic chars) <br> - Preprocessing: ?                                                                                                                                                                                                                     |                ?                |      ?     |
| [TurkuNLP/wikibert-base-de-cased](https://huggingface.co/TurkuNLP/wikibert-base-de-cased)                                                 |    BERT    |                                                                                                                                                                            - no description given | - ? batch size <br> - ? learning rate <br> - ? max sequence length <br> - ? steps <br> - ? warmup steps <br>  - 768 embeddings dimensionality <br>  - 12 heads <br> - 12 layers                                                                        | - WordPiece* based on ? <br> - 20.101 vocab size <br> - cased <br> - contains "Umlaute <br> - Preprocessing: ?                                                                                                                                                                                                                                                                              |                ?                |      ?     |


Some models found are not included in the table because they are exceptions.
These models are listed below followed by a brief explanation:
  - [anonymous-german-nlp/german-gpt2](https://huggingface.co/anonymous-german-nlp/german-gpt2) → re-released as dbmdz/german-gpt2
  - [bert-base-german-dbmdz-cased](https://huggingface.co/bert-base-german-dbmdz-cased) → re-released as dbmdz/bert-base-german-cased
  - [bert-base-german-dbmdz-uncased](https://huggingface.co/bert-base-german-dbmdz-uncased) → re-released as dbmdz/bert-base-german-uncased
  - [dbmdz/electra-base-german-europeana-cased-generator](https://huggingface.co/dbmdz/electra-base-german-europeana-cased-generator) → generator part of dbmd/electra-base-german-europeana-cased-discriminator
  - [deepset/bert-base-german-cased-oldvocab](https://huggingface.co/deepset/bert-base-german-cased-oldvocab) → origin of bert-base-german-cased but with vocab issues; use the latter
  - [deepset/gelectra-base-generator](https://huggingface.co/deepset/gelectra-base-generator) → generator part of deepset/gelectra-base
  - [deepset/gelectra-large-generator](https://huggingface.co/deepset/gelectra-large-generator) → generator part of deepset/gelectra-large
  - [timo/timo-BART-german](https://huggingface.co/timo/timo-BART-german) → doesn't seem to be a german BART model because the vocab isn't containing any german (sub)tokens.

Another exception is [Geotrend/bert-base-de-cased](https://huggingface.co/Geotrend/bert-base-de-cased).
The Geotrend team uses a completely different [approach](https://arxiv.org/abs/2010.05609) by reducing existing 
multilingual models to the vocabulary of a target language. All (sub)tokens that can be used in the target language 
(e.g. `ich, Haus, spielen` for German) are preserved. Other entries in the vocabulary (e.g. `friday, hola, 啤酒`) and 
associated connections in the Transformer are excluded.

# Domain specific Transformer language models
The given table shows Transformer language models that have been trained for use in a specific domain in German.
They can still be applied as Pre-Training models.

|                                                                 Name                                                               |                                                                                                                                                                 Raw data                                                                                                                                                                |                                                                                                       Parameters                                                                                                      |                                                          Vocab & Tokenizer                                                          |          Time         | License |
|:----------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:---------------------:|:-------:|
| [dbmdz/german-gpt2-faust](https://huggingface.co/dbmdz/german-gpt2-faust)                                                          |                                                                                                                                                                                                                       - based on dbmdz/german-gpt2 <br>   - DTA (Goethes Faust I and Faust II) = approx. 527 KB* <br>   Σ = 0,000527 GB | - 4 batch size <br> - ? learning rate <br> - 1024 max sequence length <br>  - 100 epochs <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers <br> - half precision (16 floats instead of 32 floats) | - based on dbmdz/german-gpt2 <br> - used offered "normalized" version of raw data                                                   |   12 min on RTX 3090  |   MIT   |
| [ml6team/gpt2-small-german-finetuned-oscar](https://huggingface.co/ml6team/gpt2-small-german-finetuned-oscar)                      |                                                                                                                                                                                                                                                                                   - based on ? <br>    - OSCAR* = ? GB <br>    Σ = ? GB | - ? batch size <br>  - ? learning rate <br> - ? max sequence length <br>  - ? epochs <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers                                                            | - Byte Level Byte Pair Encoding* based on ? <br> - 50.257 vocab size <br> - cased <br> - contains "Umlaute" <br> - Preprocessing: ? |           ?           |    ?    |
| [redewiedergabe/bert-base-historical-german-rw-cased](https://huggingface.co/redewiedergabe/bert-base-historical-german-rw-cased)  | - based on bert-base-german-cased <br>  - Textgrid Digitale Bibliothek (narrative texts) = ? GB <br>  - Grimm Korpus (fariry tales; sagas) = ? GB <br>  - Mannheimer Korpus (newspaper, magazines) = ? GB <br>  - Die Grenzboten (magazines) = ? GB <br>  - Projekt Gutenberg (fictional and non fictional texts) = ? GB <br>  Σ = ? GB | - 32 batch size <br> - 0.00003 learning rate <br> - 128 max sequence length <br> - 3 epochs <br> - 768 embeddings dimensionality <br> - 12 heads <br> - 12 layers                                                     | - based on bert-base-german-cased                                                                                                   |           ?           |    ?    |
| [smanjil/German-MedBERT](https://huggingface.co/smanjil/German-MedBERT)                                                            |                                                                                                                                                                                                                                                      - based on bert-base-german-cased <br>   - medical articles = ? GB <br>   Σ = ? GB | - 8 batch size <br> - 0.00005 learning rate <br> - 128 max sequence length <br> - 100.000 steps <br> - 10.000 warmup steps <br> - 768 embeddings dimensionality <br>  - 12 heads <br> - 12 layers                     | - based on bert-base-german-cased                                                                                                   | ? on Google Colab TPU |    ?    |

After the continuation of pre-training, these kind of models are subsequently fine-tuned.
A click on the respective model shows the fine-tuning performance (in case it is specified by the model's author).

<hr style="border:2px solid gray"> </hr>

## Navigation:

1. [Pre-Trained models](models_pretraining.md)
2. [Fine-Tuned models](models_finetuning.md)
3. [Datasets for custom training](models_datasets.md)
